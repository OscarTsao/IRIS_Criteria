# Training configuration

# Optimizer
optimizer:
  name: "adamw"
  lr: 1.0e-4  # Increased for larger batch size (was 2e-5)
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1.0e-8

# Learning rate scheduler
scheduler:
  name: "linear_warmup"  # or "cosine", "constant", null
  warmup_ratio: 0.1
  warmup_steps: null  # If null, use warmup_ratio

# Training
num_epochs: 20
gradient_accumulation_steps: 1  # Disabled - not needed with batch_size=64
max_grad_norm: 1.0

# Mixed precision (optimized for RTX 2080)
use_amp: true
amp_dtype: "float16"  # FP16 is faster than BF16 on RTX 2080

# Early stopping
early_stopping_patience: 5
monitor_metric: "val_loss"  # or "val_f1", "val_accuracy"
higher_is_better: false  # true for F1/accuracy, false for loss

# Checkpointing
save_best_only: true
save_checkpoint_every_n_epochs: null  # If null, only save best

# Loss function
loss:
  type: "focal"  # "bce", "weighted_bce", "focal"
  focal_gamma: 2.0
  focal_alpha: null  # Auto-computed if null
  pos_weight: null  # Auto-computed if null

# Logging
log_every_n_steps: 10
eval_every_n_epochs: 1
